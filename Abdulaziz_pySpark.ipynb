{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQGomF_rhJWJ",
        "outputId": "3c78b8fa-6e52-4bfb-aee6-41a7655136e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "---  بدأ تشغيل PySpark ---\n",
            " تم تحميل البيانات: 336776 صف\n",
            "---  تنظيف البيانات ---\n",
            "---  هندسة الميزات ---\n",
            "---  التدريب (Logistic Regression) ---\n",
            "---  النتائج ---\n",
            "\n",
            "========================================\n",
            " Accuracy (الدقة): 84.18%\n",
            " ROC-AUC (الجودة): 0.9029\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# 1. تثبيت PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# 2. الاستيراد والتهيئة\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import NumericType, StringType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Flight_Delay_Fixed\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "print(\"---  بدأ تشغيل PySpark ---\")\n",
        "\n",
        "# 3. تحميل البيانات\n",
        "try:\n",
        "    df = spark.read.csv(\"flightsData.csv\", header=True, inferSchema=True)\n",
        "    print(f\" تم تحميل البيانات: {df.count()} صف\")\n",
        "except:\n",
        "    print(\" خطأ: تأكد من رفع ملف flightsData.csv!\")\n",
        "\n",
        "# 4. تنظيف البيانات\n",
        "print(\"---  تنظيف البيانات ---\")\n",
        "\n",
        "# حذف الأعمدة غير الضرورية\n",
        "cols_to_drop = ['tailnum', 'id', 'year', 'time_hour', 'flight', 'name']\n",
        "for c in cols_to_drop:\n",
        "    if c in df.columns:\n",
        "        df = df.drop(c)\n",
        "\n",
        "# تعويض القيم المفقودة (Nulls)\n",
        "num_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]\n",
        "cat_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
        "\n",
        "# تعويض الأرقام بـ 0 والنصوص بـ Unknown\n",
        "df = df.na.fill(0, subset=num_cols)\n",
        "df = df.na.fill(\"Unknown\", subset=cat_cols)\n",
        "\n",
        "# 5. هندسة الميزات\n",
        "print(\"---  هندسة الميزات ---\")\n",
        "\n",
        "# أ) الهدف\n",
        "df = df.withColumn(\"label\", F.when(F.col(\"arr_delay\") > 0, 1.0).otherwise(0.0))\n",
        "\n",
        "# ب) اللوغاريتم (مع حماية من القيم السالبة والصفر)\n",
        "# نستخدم when للتأكد أننا لا نأخذ لوغاريتم لرقم <= 0\n",
        "df = df.withColumn(\"dep_delay_log\",\n",
        "                   F.when(F.col(\"dep_delay\") > 0, F.log1p(F.col(\"dep_delay\")))\n",
        "                   .otherwise(0.0))\n",
        "\n",
        "# ج) مراحل Pipeline\n",
        "stages = []\n",
        "\n",
        "# تحويل النصوص\n",
        "for c in cat_cols:\n",
        "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") # keep مهم جداً\n",
        "    encoder = OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_vec\")\n",
        "    stages += [indexer, encoder]\n",
        "\n",
        "# تجميع الميزات\n",
        "exclude = ['label', 'arr_delay'] + cat_cols\n",
        "input_cols = [c for c in df.columns if c not in exclude]\n",
        "final_cols = input_cols + [f\"{c}_vec\" for c in cat_cols]\n",
        "\n",
        "# التعديل هنا: handleInvalid=\"skip\"\n",
        "# هذا يمنع الخطأ ويخبر Spark بتجاهل أي صف فيه مشكلة بدلاً من إيقاف الكود\n",
        "assembler = VectorAssembler(inputCols=final_cols, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\")\n",
        "stages += [assembler, scaler]\n",
        "\n",
        "# 6. التدريب\n",
        "print(\"---  التدريب (Logistic Regression) ---\")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"label\", maxIter=20)\n",
        "stages.append(lr)\n",
        "\n",
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "# تقسيم البيانات\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Fit\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# 7. النتائج\n",
        "print(\"---  النتائج ---\")\n",
        "predictions = model.transform(test)\n",
        "\n",
        "acc = MulticlassClassificationEvaluator(metricName=\"accuracy\").evaluate(predictions)\n",
        "auc = BinaryClassificationEvaluator(metricName=\"areaUnderROC\").evaluate(predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\" Accuracy (الدقة): {acc:.2%}\")\n",
        "print(f\" ROC-AUC (الجودة): {auc:.4f}\")\n",
        "print(\"=\"*40)"
      ]
    }
  ]
}